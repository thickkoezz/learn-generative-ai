{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('$9.4 million', 'MONEY'), ('the prior year', 'DATE'), ('$2.7 million', 'MONEY')]\n",
      "[('twelve billion dollars', 'MONEY'), ('1b', 'MONEY')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "texts = [\n",
    "    \"Net income was $9.4 million compared to the prior year of $2.7 million.\",\n",
    "    \"Revenue exceeded twelve billion dollars, with a loss of $1b.\",\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "for doc in nlp.pipe(texts, disable=[\"tok2vec\", \"tagger\", \"parser\", \"attribute_ruler\", \"lemmatizer\"]):\n",
    "    # Do something with the doc here\n",
    "    print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text1: This is the first text.\n",
      "text2: This is the second text.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc\n",
    "\n",
    "if not Doc.has_extension(\"text_id\"):\n",
    "    Doc.set_extension(\"text_id\", default=None)\n",
    "\n",
    "text_tuples = [\n",
    "    (\"This is the first text.\", {\"text_id\": \"text1\"}),\n",
    "    (\"This is the second text.\", {\"text_id\": \"text2\"})\n",
    "]\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc_tuples = nlp.pipe(text_tuples, as_tuples=True)\n",
    "\n",
    "docs = []\n",
    "for doc, context in doc_tuples:\n",
    "    doc._.text_id = context[\"text_id\"]\n",
    "    docs.append(doc)\n",
    "\n",
    "for doc in docs:\n",
    "    print(f\"{doc._.text_id}: {doc.text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\"This is a text\", \"These are lots of texts\", \"...\"]\n",
    "\n",
    "# Multiprocessing with 4 processes\n",
    "docs = nlp.pipe(texts, n_process=4)\n",
    "\n",
    "# With as many processes as CPUs (use with caution!)\n",
    "docs = nlp.pipe(texts, n_process=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default batch size is `nlp.batch_size` (typically 1000)\n",
    "docs = nlp.pipe(texts, n_process=2, batch_size=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spacy.load under the hood (abstract example)\n",
    "lang = \"en\"\n",
    "pipeline = [\"tok2vec\", \"tagger\", \"parser\", \"ner\", \"attribute_ruler\", \"lemmatizer\"]\n",
    "data_path = \"path/to/en_core_web_sm/en_core_web_sm-3.0.0\"\n",
    "\n",
    "cls = spacy.util.get_lang_class(lang)  # 1. Get Language class, e.g. English\n",
    "nlp = cls()                            # 2. Initialize it\n",
    "# for name in pipeline:\n",
    "#     nlp.add_pipe(name, config={...})   # 3. Add the component to the pipeline\n",
    "# nlp.from_disk(data_path)               # 4. Load in the binary data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pipeline under the hood\n",
    "doc = nlp.make_doc(\"This is a sentence\")  # Create a Doc from raw text\n",
    "for name, proc in nlp.pipeline:           # Iterate over components in order\n",
    "    doc = proc(doc)                       # Apply each component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipeline)\n",
    "# [('tok2vec', <spacy.pipeline.Tok2Vec>), ('tagger', <spacy.pipeline.Tagger>), ('parser', <spacy.pipeline.DependencyParser>), ('ner', <spacy.pipeline.EntityRecognizer>), ('attribute_ruler', <spacy.pipeline.AttributeRuler>), ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer>)]\n",
    "print(nlp.pipe_names)\n",
    "# ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pipeline without the entity recognizer\n",
    "nlp = spacy.load(\"en_core_web_sm\", exclude=[\"ner\"])\n",
    "\n",
    "# Load the tagger and parser but don't enable them\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\"])\n",
    "# Explicitly enable the tagger later on\n",
    "nlp.enable_pipe(\"tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E1042] `enable=['ner']` and `disable=['ner', 'senter']` are inconsistent with each other.\nIf you only passed one of `enable` or `disable`, the other argument is specified in your pipeline's configuration.\nIn that case pass an empty list for the previously not specified argument to avoid this error.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m, enable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtok2vec\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtagger\u001b[39m\u001b[38;5;124m\"\u001b[39m], disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Will raise an error, as the sets of enabled and disabled components are conflicting\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m nlp \u001b[38;5;241m=\u001b[39m spacy\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m\"\u001b[39m, enable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m], disable\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\n\u001b[0;32m     28\u001b[0m     name: Union[\u001b[38;5;28mstr\u001b[39m, Path],\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;241m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     34\u001b[0m     config: Union[Dict[\u001b[38;5;28mstr\u001b[39m, Any], Config] \u001b[38;5;241m=\u001b[39m util\u001b[38;5;241m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     35\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Language:\n\u001b[0;32m     36\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \n\u001b[0;32m     38\u001b[0m \u001b[38;5;124;03m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;124;03m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     50\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m util\u001b[38;5;241m.\u001b[39mload_model(\n\u001b[0;32m     52\u001b[0m         name,\n\u001b[0;32m     53\u001b[0m         vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m     54\u001b[0m         disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m     55\u001b[0m         enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m     56\u001b[0m         exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m     57\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m     58\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:465\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_lang_class(name\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblank:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))()\n\u001b[0;32m    464\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_package(name):  \u001b[38;5;66;03m# installed as package\u001b[39;00m\n\u001b[1;32m--> 465\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_package(name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m Path(name)\u001b[38;5;241m.\u001b[39mexists():  \u001b[38;5;66;03m# path to model data directory\u001b[39;00m\n\u001b[0;32m    467\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(Path(name), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:501\u001b[0m, in \u001b[0;36mload_model_from_package\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load a model from an installed package.\u001b[39;00m\n\u001b[0;32m    485\u001b[0m \n\u001b[0;32m    486\u001b[0m \u001b[38;5;124;03mname (str): The package name.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;124;03mRETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    500\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m importlib\u001b[38;5;241m.\u001b[39mimport_module(name)\n\u001b[1;32m--> 501\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mload(vocab\u001b[38;5;241m=\u001b[39mvocab, disable\u001b[38;5;241m=\u001b[39mdisable, enable\u001b[38;5;241m=\u001b[39menable, exclude\u001b[38;5;241m=\u001b[39mexclude, config\u001b[38;5;241m=\u001b[39mconfig)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\en_core_web_sm\\__init__.py:10\u001b[0m, in \u001b[0;36mload\u001b[1;34m(**overrides)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides):\n\u001b[1;32m---> 10\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_init_py(\u001b[38;5;18m__file__\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:682\u001b[0m, in \u001b[0;36mload_model_from_init_py\u001b[1;34m(init_file, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path\u001b[38;5;241m.\u001b[39mexists():\n\u001b[0;32m    681\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE052\u001b[38;5;241m.\u001b[39mformat(path\u001b[38;5;241m=\u001b[39mdata_path))\n\u001b[1;32m--> 682\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m load_model_from_path(\n\u001b[0;32m    683\u001b[0m     data_path,\n\u001b[0;32m    684\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m    685\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m    686\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m    687\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m    688\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m    689\u001b[0m     config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    690\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:539\u001b[0m, in \u001b[0;36mload_model_from_path\u001b[1;34m(model_path, meta, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    537\u001b[0m overrides \u001b[38;5;241m=\u001b[39m dict_to_dot(config, for_overrides\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    538\u001b[0m config \u001b[38;5;241m=\u001b[39m load_config(config_path, overrides\u001b[38;5;241m=\u001b[39moverrides)\n\u001b[1;32m--> 539\u001b[0m nlp \u001b[38;5;241m=\u001b[39m load_model_from_config(\n\u001b[0;32m    540\u001b[0m     config,\n\u001b[0;32m    541\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m    542\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m    543\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m    544\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m    545\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m    546\u001b[0m )\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\u001b[38;5;241m.\u001b[39mfrom_disk(model_path, exclude\u001b[38;5;241m=\u001b[39mexclude, overrides\u001b[38;5;241m=\u001b[39moverrides)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\util.py:587\u001b[0m, in \u001b[0;36mload_model_from_config\u001b[1;34m(config, meta, vocab, disable, enable, exclude, auto_fill, validate)\u001b[0m\n\u001b[0;32m    584\u001b[0m \u001b[38;5;66;03m# This will automatically handle all codes registered via the languages\u001b[39;00m\n\u001b[0;32m    585\u001b[0m \u001b[38;5;66;03m# registry, including custom subclasses provided via entry points\u001b[39;00m\n\u001b[0;32m    586\u001b[0m lang_cls \u001b[38;5;241m=\u001b[39m get_lang_class(nlp_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlang\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m--> 587\u001b[0m nlp \u001b[38;5;241m=\u001b[39m lang_cls\u001b[38;5;241m.\u001b[39mfrom_config(\n\u001b[0;32m    588\u001b[0m     config,\n\u001b[0;32m    589\u001b[0m     vocab\u001b[38;5;241m=\u001b[39mvocab,\n\u001b[0;32m    590\u001b[0m     disable\u001b[38;5;241m=\u001b[39mdisable,\n\u001b[0;32m    591\u001b[0m     enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m    592\u001b[0m     exclude\u001b[38;5;241m=\u001b[39mexclude,\n\u001b[0;32m    593\u001b[0m     auto_fill\u001b[38;5;241m=\u001b[39mauto_fill,\n\u001b[0;32m    594\u001b[0m     validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    595\u001b[0m     meta\u001b[38;5;241m=\u001b[39mmeta,\n\u001b[0;32m    596\u001b[0m )\n\u001b[0;32m    597\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m nlp\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\language.py:1976\u001b[0m, in \u001b[0;36mLanguage.from_config\u001b[1;34m(cls, config, vocab, disable, enable, exclude, meta, auto_fill, validate)\u001b[0m\n\u001b[0;32m   1968\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m   1969\u001b[0m             Warnings\u001b[38;5;241m.\u001b[39mW123\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1970\u001b[0m                 enable\u001b[38;5;241m=\u001b[39menable,\n\u001b[0;32m   1971\u001b[0m                 enabled\u001b[38;5;241m=\u001b[39menabled,\n\u001b[0;32m   1972\u001b[0m             )\n\u001b[0;32m   1973\u001b[0m         )\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;66;03m# Ensure sets of disabled/enabled pipe names are not contradictory.\u001b[39;00m\n\u001b[1;32m-> 1976\u001b[0m disabled_pipes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_resolve_component_status(\n\u001b[0;32m   1977\u001b[0m     \u001b[38;5;28mlist\u001b[39m({\u001b[38;5;241m*\u001b[39mdisable, \u001b[38;5;241m*\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled\u001b[39m\u001b[38;5;124m\"\u001b[39m, [])}),\n\u001b[0;32m   1978\u001b[0m     enable,\n\u001b[0;32m   1979\u001b[0m     config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpipeline\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   1980\u001b[0m )\n\u001b[0;32m   1981\u001b[0m nlp\u001b[38;5;241m.\u001b[39m_disabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(p \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m disabled_pipes \u001b[38;5;28;01mif\u001b[39;00m p \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m exclude)\n\u001b[0;32m   1983\u001b[0m nlp\u001b[38;5;241m.\u001b[39mbatch_size \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlp\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\spacy\\language.py:2190\u001b[0m, in \u001b[0;36mLanguage._resolve_component_status\u001b[1;34m(disable, enable, pipe_names)\u001b[0m\n\u001b[0;32m   2188\u001b[0m     \u001b[38;5;66;03m# If any pipe to be enabled is in to_disable, the specification is inconsistent.\u001b[39;00m\n\u001b[0;32m   2189\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(enable) \u001b[38;5;241m&\u001b[39m to_disable):\n\u001b[1;32m-> 2190\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(Errors\u001b[38;5;241m.\u001b[39mE1042\u001b[38;5;241m.\u001b[39mformat(enable\u001b[38;5;241m=\u001b[39menable, disable\u001b[38;5;241m=\u001b[39mdisable))\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(to_disable)\n",
      "\u001b[1;31mValueError\u001b[0m: [E1042] `enable=['ner']` and `disable=['ner', 'senter']` are inconsistent with each other.\nIf you only passed one of `enable` or `disable`, the other argument is specified in your pipeline's configuration.\nIn that case pass an empty list for the previously not specified argument to avoid this error."
     ]
    }
   ],
   "source": [
    "# Load the complete pipeline, but disable all components except for tok2vec and tagger\n",
    "nlp = spacy.load(\"en_core_web_sm\", enable=[\"tok2vec\", \"tagger\"])\n",
    "# Has the same effect, as NER is already not part of enabled set of components\n",
    "nlp = spacy.load(\"en_core_web_sm\", enable=[\"tok2vec\", \"tagger\"], disable=[\"ner\"])\n",
    "# Will raise an error, as the sets of enabled and disabled components are conflicting\n",
    "nlp = spacy.load(\"en_core_web_sm\", enable=[\"ner\"], disable=[\"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Use as a context manager\n",
    "with nlp.select_pipes(disable=[\"tagger\", \"parser\", \"lemmatizer\"]):\n",
    "    doc = nlp(\"I won't be tagged and parsed\")\n",
    "doc = nlp(\"I will be tagged and parsed\")\n",
    "\n",
    "# 2. Restore manually\n",
    "disabled = nlp.select_pipes(disable=\"ner\")\n",
    "doc = nlp(\"I won't have named entities\")\n",
    "disabled.restore()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable only the parser\n",
    "with nlp.select_pipes(enable=\"parser\"):\n",
    "    doc = nlp(\"I will only be parsed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for doc in nlp.pipe(texts, disable=[\"tagger\", \"parser\", \"lemmatizer\"]):\n",
    "    pass # Do something with the doc here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.remove_pipe(\"parser\")\n",
    "nlp.rename_pipe(\"ner\", \"entityrecognizer\")\n",
    "nlp.replace_pipe(\"tagger\", \"my_custom_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# The source pipeline with different components\n",
    "source_nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(source_nlp.pipe_names)\n",
    "\n",
    "# Add only the entity recognizer to the new blank pipeline\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"ner\", source=source_nlp)\n",
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"tagger\")\n",
    "# This is a problem because it needs entities and sentence boundaries\n",
    "nlp.add_pipe(\"entity_linker\")\n",
    "analysis = nlp.analyze_pipes(pretty=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"info_component\")\n",
    "def my_component(doc):\n",
    "    print(f\"After tokenization, this doc has {len(doc)} tokens.\")\n",
    "    print(\"The part-of-speech tags are:\", [token.pos_ for token in doc])\n",
    "    if len(doc) < 10:\n",
    "        print(\"This is a pretty short document.\")\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"info_component\", name=\"print_info\", last=True)\n",
    "print(nlp.pipe_names)  # ['tagger', 'parser', 'ner', 'print_info']\n",
    "doc = nlp(\"This is a sentence.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_sentencizer\")\n",
    "def custom_sentencizer(doc):\n",
    "    for i, token in enumerate(doc[:-2]):\n",
    "        # Define sentence start if pipe + titlecase token\n",
    "        if token.text == \"|\" and doc[i + 1].is_title:\n",
    "            doc[i + 1].is_sent_start = True\n",
    "        else:\n",
    "            # Explicitly set sentence start to False otherwise, to tell\n",
    "            # the parser to leave those tokens alone\n",
    "            doc[i + 1].is_sent_start = False\n",
    "    return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"custom_sentencizer\", before=\"parser\")  # Insert before the parser\n",
    "doc = nlp(\"This is. A sentence. | This is. Another sentence.\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.factory(\"my_component\", default_config={\"some_setting\": True})\n",
    "def my_component(nlp, name, some_setting: bool):\n",
    "    return MyComponent(some_setting=some_setting)\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"my_component\", config={\"some_setting\": False})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.lang.de import German\n",
    "\n",
    "class TokenNormalizer:\n",
    "    def __init__(self, norm_table):\n",
    "        self.norm_table = norm_table\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        for token in doc:\n",
    "            # Overwrite the token.norm_ if there's an entry in the data\n",
    "            token.norm_ = self.norm_table.get(token.text, token.norm_)\n",
    "        return doc\n",
    "\n",
    "@English.factory(\"token_normalizer\")\n",
    "def create_en_normalizer(nlp, name):\n",
    "    return TokenNormalizer({\"realise\": \"realize\", \"colour\": \"color\"})\n",
    "\n",
    "@German.factory(\"token_normalizer\")\n",
    "def create_de_normalizer(nlp, name):\n",
    "    return TokenNormalizer({\"daß\": \"dass\", \"wußte\": \"wusste\"})\n",
    "\n",
    "nlp_en = English()\n",
    "nlp_en.add_pipe(\"token_normalizer\")  # uses the English factory\n",
    "print([token.norm_ for token in nlp_en(\"realise colour daß wußte\")])\n",
    "\n",
    "nlp_de = German()\n",
    "nlp_de.add_pipe(\"token_normalizer\")  # uses the German factory\n",
    "print([token.norm_ for token in nlp_de(\"realise colour daß wußte\")])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from spacy.matcher import PhraseMatcher\n",
    "import spacy\n",
    "\n",
    "DICTIONARY = {\"lol\": \"laughing out loud\", \"brb\": \"be right back\"}\n",
    "DICTIONARY.update({value: key for key, value in DICTIONARY.items()})\n",
    "\n",
    "@Language.factory(\"acronyms\", default_config={\"case_sensitive\": False})\n",
    "def create_acronym_component(nlp: Language, name: str, case_sensitive: bool):\n",
    "    return AcronymComponent(nlp, case_sensitive)\n",
    "\n",
    "class AcronymComponent:\n",
    "    def __init__(self, nlp: Language, case_sensitive: bool):\n",
    "        # Create the matcher and match on Token.lower if case-insensitive\n",
    "        matcher_attr = \"TEXT\" if case_sensitive else \"LOWER\"\n",
    "        self.matcher = PhraseMatcher(nlp.vocab, attr=matcher_attr)\n",
    "        self.matcher.add(\"ACRONYMS\", [nlp.make_doc(term) for term in DICTIONARY])\n",
    "        self.case_sensitive = case_sensitive\n",
    "        # Register custom extension on the Doc\n",
    "        if not Doc.has_extension(\"acronyms\"):\n",
    "            Doc.set_extension(\"acronyms\", default=[])\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        # Add the matched spans when doc is processed\n",
    "        for _, start, end in self.matcher(doc):\n",
    "            span = doc[start:end]\n",
    "            acronym = DICTIONARY.get(span.text if self.case_sensitive else span.text.lower())\n",
    "            doc._.acronyms.append((span, acronym))\n",
    "        return doc\n",
    "\n",
    "# Add the component to the pipeline and configure it\n",
    "nlp = spacy.blank(\"en\")\n",
    "nlp.add_pipe(\"acronyms\", config={\"case_sensitive\": False})\n",
    "\n",
    "# Process a doc and see the results\n",
    "doc = nlp(\"LOL, be right back\")\n",
    "print(doc._.acronyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"acronyms\", default_config={\"data\": {}, \"case_sensitive\": False})\n",
    "def create_acronym_component(nlp: Language, name: str, data: Dict[str, str], case_sensitive: bool):\n",
    "    # 🚨 Problem: data ends up in the config file\n",
    "    return AcronymComponent(nlp, data, case_sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registered function for assets\n",
    "@spacy.registry.misc(\"acronyms.slang_dict.v1\")\n",
    "def create_acronyms_slang_dict():\n",
    "    dictionary = {\"lol\": \"laughing out loud\", \"brb\": \"be right back\"}\n",
    "    dictionary.update({value: key for key, value in dictionary.items()})\n",
    "    return dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom serialization methods\n",
    "import srsly\n",
    "from spacy.util import ensure_path\n",
    "\n",
    "class AcronymComponent:\n",
    "    # other methods here...\n",
    "\n",
    "    def to_disk(self, path, exclude=tuple()):\n",
    "        path = ensure_path(path)\n",
    "        if not path.exists():\n",
    "            path.mkdir()\n",
    "        srsly.write_json(path / \"data.json\", self.data)\n",
    "\n",
    "    def from_disk(self, path, exclude=tuple()):\n",
    "        self.data = srsly.read_json(path / \"data.json\")\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"acronyms\", default_config={\"data\": {}, \"case_sensitive\": False})\n",
    "def create_acronym_component(nlp: Language, name: str, data: Dict[str, str], case_sensitive: bool):\n",
    "    # 🚨 Problem: data will be loaded every time component is created\n",
    "    return AcronymComponent(nlp, data, case_sensitive)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom initialize method\n",
    "class AcronymComponent:\n",
    "    def __init__(self):\n",
    "        self.data = {}\n",
    "\n",
    "    def initialize(self, get_examples=None, nlp=None, data={}):\n",
    "        self.data = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Doc\n",
    "from pydantic import StrictStr\n",
    "import logging\n",
    "\n",
    "@Language.factory(\"debug\", default_config={\"log_level\": \"DEBUG\"})\n",
    "class DebugComponent:\n",
    "    def __init__(self, nlp: Language, name: str, log_level: StrictStr):\n",
    "        self.logger = logging.getLogger(f\"spacy.{name}\")\n",
    "        self.logger.setLevel(log_level)\n",
    "        self.logger.info(f\"Pipeline: {nlp.pipe_names}\")\n",
    "\n",
    "    def __call__(self, doc: Doc) -> Doc:\n",
    "        is_tagged = doc.has_annotation(\"TAG\")\n",
    "        self.logger.debug(f\"Doc: {len(doc)} tokens, is tagged: {is_tagged}\")\n",
    "        return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"debug\", config={\"log_level\": \"DEBUG\"})\n",
    "doc = nlp(\"This is a text...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension(\"hello\", default=True)\n",
    "assert doc._.hello\n",
    "doc._.hello = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension(\"hello\", getter=get_hello_value, setter=set_hello_value)\n",
    "assert doc._.hello\n",
    "doc._.hello = \"Hi!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Doc.set_extension(\"hello\", method=lambda doc, name: f\"Hi {name}!\")\n",
    "assert doc._.hello(\"Bob\") == \"Hi Bob!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "fruits = [\"apple\", \"pear\", \"banana\", \"orange\", \"strawberry\"]\n",
    "is_fruit_getter = lambda token: token.text in fruits\n",
    "has_fruit_getter = lambda obj: any([t.text in fruits for t in obj])\n",
    "\n",
    "Token.set_extension(\"is_fruit\", getter=is_fruit_getter)\n",
    "Doc.set_extension(\"has_fruit\", getter=has_fruit_getter)\n",
    "Span.set_extension(\"has_fruit\", getter=has_fruit_getter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from spacy.lang.en import English\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Doc, Span, Token\n",
    "\n",
    "@Language.factory(\"rest_countries\")\n",
    "class RESTCountriesComponent:\n",
    "    def __init__(self, nlp, name, label=\"GPE\"):\n",
    "        r = requests.get(\"https://restcountries.com/v2/all\")\n",
    "        r.raise_for_status()  # make sure requests raises an error if it fails\n",
    "        countries = r.json()\n",
    "        # Convert API response to dict keyed by country name for easy lookup\n",
    "        self.countries = {c[\"name\"]: c for c in countries}\n",
    "        self.label = label\n",
    "        # Set up the PhraseMatcher with Doc patterns for each country name\n",
    "        self.matcher = PhraseMatcher(nlp.vocab)\n",
    "        self.matcher.add(\"COUNTRIES\", [nlp.make_doc(c) for c in self.countries.keys()])\n",
    "        # Register attributes on the Span. We'll be overwriting this based on\n",
    "        # the matches, so we're only setting a default value, not a getter.\n",
    "        Span.set_extension(\"is_country\", default=None)\n",
    "        Span.set_extension(\"country_capital\", default=None)\n",
    "        Span.set_extension(\"country_latlng\", default=None)\n",
    "        Span.set_extension(\"country_flag\", default=None)\n",
    "        # Register attribute on Doc via a getter that checks if the Doc\n",
    "        # contains a country entity\n",
    "        Doc.set_extension(\"has_country\", getter=self.has_country)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        spans = []  # keep the spans for later so we can merge them afterwards\n",
    "        for _, start, end in self.matcher(doc):\n",
    "            # Generate Span representing the entity & set label\n",
    "            entity = Span(doc, start, end, label=self.label)\n",
    "            # Set custom attributes on entity. Can be extended with other data\n",
    "            # returned by the API, like currencies, country code, calling code etc.\n",
    "            entity._.set(\"is_country\", True)\n",
    "            entity._.set(\"country_capital\", self.countries[entity.text][\"capital\"])\n",
    "            entity._.set(\"country_latlng\", self.countries[entity.text][\"latlng\"])\n",
    "            entity._.set(\"country_flag\", self.countries[entity.text][\"flag\"])\n",
    "            spans.append(entity)\n",
    "        # Overwrite doc.ents and add entity – be careful not to replace!\n",
    "        doc.ents = list(doc.ents) + spans\n",
    "        return doc  # don't forget to return the Doc!\n",
    "\n",
    "    def has_country(self, doc):\n",
    "        \"\"\"Getter for Doc attributes. Since the getter is only called\n",
    "        when we access the attribute, we can refer to the Span's 'is_country'\n",
    "        attribute here, which is already set in the processing step.\"\"\"\n",
    "        return any([entity._.get(\"is_country\") for entity in doc.ents])\n",
    "\n",
    "nlp = English()\n",
    "nlp.add_pipe(\"rest_countries\", config={\"label\": \"GPE\"})\n",
    "doc = nlp(\"Some text about Colombia and the Czech Republic\")\n",
    "print(\"Pipeline\", nlp.pipe_names)  # pipeline contains component name\n",
    "print(\"Doc has countries\", doc._.has_country)  # Doc contains countries\n",
    "for ent in doc.ents:\n",
    "    if ent._.is_country:\n",
    "        print(ent.text, ent.label_, ent._.country_capital, ent._.country_latlng, ent._.country_flag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add custom similarity hooks\n",
    "from spacy.language import Language\n",
    "\n",
    "\n",
    "class SimilarityModel:\n",
    "    def __init__(self, name: str, index: int):\n",
    "        self.name = name\n",
    "        self.index = index\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        doc.user_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_span_hooks[\"similarity\"] = self.similarity\n",
    "        doc.user_token_hooks[\"similarity\"] = self.similarity\n",
    "        return doc\n",
    "\n",
    "    def similarity(self, obj1, obj2):\n",
    "        return obj1.vector[self.index] + obj2.vector[self.index]\n",
    "\n",
    "\n",
    "@Language.factory(\"similarity_component\", default_config={\"index\": 0})\n",
    "def create_similarity_component(nlp, name, index: int):\n",
    "    return SimilarityModel(name, index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import your_custom_entity_recognizer\n",
    "from spacy.training import biluo_tags_to_spans\n",
    "from spacy.language import Language\n",
    "\n",
    "@Language.component(\"custom_ner_wrapper\")\n",
    "def custom_ner_wrapper(doc):\n",
    "    words = [token.text for token in doc]\n",
    "    custom_entities = your_custom_entity_recognizer(words)\n",
    "    doc.ents = biluo_tags_to_spans(doc, custom_entities)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import your_custom_model\n",
    "from spacy.language import Language\n",
    "from spacy.symbols import POS, TAG, DEP, HEAD\n",
    "from spacy.tokens import Doc\n",
    "import numpy\n",
    "\n",
    "@Language.component(\"custom_model_wrapper\")\n",
    "def custom_model_wrapper(doc):\n",
    "    words = [token.text for token in doc]\n",
    "    spaces = [token.whitespace for token in doc]\n",
    "    pos, tags, deps, heads = your_custom_model(words)\n",
    "    # Convert the strings to integers and add them to the string store\n",
    "    pos = [doc.vocab.strings.add(label) for label in pos]\n",
    "    tags = [doc.vocab.strings.add(label) for label in tags]\n",
    "    deps = [doc.vocab.strings.add(label) for label in deps]\n",
    "    # Create a new Doc from a numpy array\n",
    "    attrs = [POS, TAG, DEP, HEAD]\n",
    "    arr = numpy.array(list(zip(pos, tags, deps, heads)), dtype=\"uint64\")\n",
    "    new_doc = Doc(doc.vocab, words=words, spaces=spaces).from_array(attrs, arr)\n",
    "    return new_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heads = [2, 0, 4, 2, 2]\n",
    "new_heads = [head - i - 1 if head != 0 else 0 for i, head in enumerate(heads)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
