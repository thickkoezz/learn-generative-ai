{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "# Add match ID \"HelloWorld\" with no callback and one pattern\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "\n",
    "doc = nlp(\"Hello, world! Hello world!\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # Get string representation\n",
    "    span = doc[start:end]  # The matched span\n",
    "    print(match_id, string_id, start, end, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for match_id, start, end in matches:\n",
    "    string_id = nlp.vocab.strings[match_id]  # 'HelloWorld'\n",
    "    span = doc[start:end]                    # The matched span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = [\n",
    "    [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"LOWER\": \"world\"}],\n",
    "    [{\"LOWER\": \"hello\"}, {\"LOWER\": \"world\"}]\n",
    "]\n",
    "matcher.add(\"HelloWorld\", patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"REGEX\": \"^[Uu](\\\\.?|nited)$\"}},\n",
    "           {\"TEXT\": {\"REGEX\": \"^[Ss](\\\\.?|tates)$\"}},\n",
    "           {\"LOWER\": \"president\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match different spellings of token texts\n",
    "pattern = [{\"TEXT\": {\"REGEX\": \"deff?in[ia]tely\"}}]\n",
    "\n",
    "# Match tokens with fine-grained POS tags starting with 'V'\n",
    "pattern = [{\"TAG\": {\"REGEX\": \"^V\"}}]\n",
    "\n",
    "# Match custom attribute values with regular expressions\n",
    "pattern = [{\"_\": {\"country\": {\"REGEX\": \"^[Uu](nited|\\\\.?) ?[Ss](tates|\\\\.?)$\"}}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"The United States of America (USA) are commonly known as the United States (U.S. or US) or America.\")\n",
    "\n",
    "expression = r\"[Uu](nited|\\.?) ?[Ss](tates|\\.?)\"\n",
    "for match in re.finditer(expression, doc.text):\n",
    "    start, end = match.span()\n",
    "    span = doc.char_span(start, end)\n",
    "    # This is a Span object or None if match doesn't map to valid token sequence\n",
    "    if span is not None:\n",
    "        print(\"Found match:\", span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matches \"favourite\", \"favorites\", \"gavorite\", \"theatre\", \"theatr\", ...\n",
    "pattern = [{\"TEXT\": {\"FUZZY\": \"favorite\"}},\n",
    "           {\"TEXT\": {\"FUZZY\": \"theater\"}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match lowercase with fuzzy matching (allows 3 edits)\n",
    "pattern = [{\"LOWER\": {\"FUZZY\": \"definitely\"}}]\n",
    "\n",
    "# Match custom attribute values with fuzzy matching (allows 3 edits)\n",
    "pattern = [{\"_\": {\"country\": {\"FUZZY\": \"Kyrgyzstan\"}}}]\n",
    "\n",
    "# Match with exact Levenshtein edit distance limits (allows 4 edits)\n",
    "pattern = [{\"_\": {\"country\": {\"FUZZY4\": \"Kyrgyzstan\"}}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = [{\"TEXT\": {\"FUZZY\": {\"IN\": [\"awesome\", \"cool\", \"wonderful\"]}}}]\n",
    "\n",
    "pattern = [{\"TEXT\": {\"REGEX\": {\"NOT_IN\": [\"^awe(some)?$\", \"^wonder(ful)?\"]}}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{\"ORTH\": \"User\"}, {\"ORTH\": \"name\"}, {\"ORTH\": \":\"}, {}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab, validate=True)\n",
    "# Add match ID \"HelloWorld\" with unsupported attribute CASEINSENSITIVE\n",
    "pattern = [{\"LOWER\": \"hello\"}, {\"IS_PUNCT\": True}, {\"CASEINSENSITIVE\": \"world\"}]\n",
    "matcher.add(\"HelloWorld\", [pattern])\n",
    "# 🚨 Raises an error:\n",
    "# MatchPatternError: Invalid token patterns for matcher rule 'HelloWorld'\n",
    "# Pattern 0:\n",
    "# - [pattern -> 2 -> CASEINSENSITIVE] extra fields not permitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = English()\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "def add_event_ent(matcher, doc, i, matches):\n",
    "    # Get the current match and create tuple of entity label, start and end.\n",
    "    # Append entity to the doc's entity. (Don't overwrite doc.ents!)\n",
    "    match_id, start, end = matches[i]\n",
    "    entity = Span(doc, start, end, label=\"EVENT\")\n",
    "    doc.ents += (entity,)\n",
    "    print(entity.text)\n",
    "\n",
    "pattern = [{\"ORTH\": \"Google\"}, {\"ORTH\": \"I\"}, {\"ORTH\": \"/\"}, {\"ORTH\": \"O\"}]\n",
    "matcher.add(\"GoogleIO\", [pattern], on_match=add_event_ent)\n",
    "doc = nlp(\"This is a text about Google I/O\")\n",
    "matches = matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matcher.add(\"PERSON\", [[{\"lower\": \"barack\"}, {\"lower\": \"obama\"}]])\n",
    "doc = nlp(\"Barack Obama was the 44th president of the United States\")\n",
    "\n",
    "# 1. Return (match_id, start, end) tuples\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    # Create the matched span and assign the match_id as a label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    print(span.text, span.label_)\n",
    "\n",
    "# 2. Return Span objects directly\n",
    "matches = matcher(doc, as_spans=True)\n",
    "for span in matches:\n",
    "    print(span.text, span.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(YOUR_TEXT_HERE)\n",
    "matcher(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "\n",
    "# We're using a component factory because the component needs to be\n",
    "# initialized with the shared vocab via the nlp object\n",
    "@Language.factory(\"html_merger\")\n",
    "def create_bad_html_merger(nlp, name):\n",
    "    return BadHTMLMerger(nlp.vocab)\n",
    "\n",
    "class BadHTMLMerger:\n",
    "    def __init__(self, vocab):\n",
    "        patterns = [\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"br\"}, {\"ORTH\": \">\"}],\n",
    "            [{\"ORTH\": \"<\"}, {\"LOWER\": \"br/\"}, {\"ORTH\": \">\"}],\n",
    "        ]\n",
    "        # Register a new token extension to flag bad HTML\n",
    "        Token.set_extension(\"bad_html\", default=False)\n",
    "        self.matcher = Matcher(vocab)\n",
    "        self.matcher.add(\"BAD_HTML\", patterns)\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        # This method is invoked when the component is called on a Doc\n",
    "        matches = self.matcher(doc)\n",
    "        spans = []  # Collect the matched spans here\n",
    "        for match_id, start, end in matches:\n",
    "            spans.append(doc[start:end])\n",
    "        with doc.retokenize() as retokenizer:\n",
    "            for span in spans:\n",
    "                retokenizer.merge(span)\n",
    "                for token in span:\n",
    "                    token._.bad_html = True  # Mark token as bad HTML\n",
    "        return doc\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"html_merger\", last=True)  # Add component to the pipeline\n",
    "doc = nlp(\"Hello<br>world! <br/> This is a test.\")\n",
    "for token in doc:\n",
    "    print(token.text, token._.bad_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory(\"html_merger\", default_config={\"path\": None})\n",
    "def create_bad_html_merger(nlp, name, path):\n",
    "    return BadHTMLMerger(nlp, path=path)\n",
    "\n",
    "nlp.add_pipe(\"html_merger\", config={\"path\": \"/path/to/patterns.json\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"}, {\"POS\": \"ADJ\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "matched_sents = []  # Collect data of matched sentences to be visualized\n",
    "\n",
    "def collect_sents(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    span = doc[start:end]  # Matched span\n",
    "    sent = span.sent  # Sentence containing matched span\n",
    "    # Append mock entity for match in displaCy style to matched_sents\n",
    "    # get the match span by ofsetting the start and end of the span with the\n",
    "    # start and end of the sentence in the doc\n",
    "    match_ents = [{\n",
    "        \"start\": span.start_char - sent.start_char,\n",
    "        \"end\": span.end_char - sent.start_char,\n",
    "        \"label\": \"MATCH\",\n",
    "    }]\n",
    "    matched_sents.append({\"text\": sent.text, \"ents\": match_ents})\n",
    "\n",
    "pattern = [{\"LOWER\": \"facebook\"}, {\"LEMMA\": \"be\"}, {\"POS\": \"ADV\", \"OP\": \"*\"},\n",
    "           {\"POS\": \"ADJ\"}]\n",
    "matcher.add(\"FacebookIs\", [pattern], on_match=collect_sents)  # add pattern\n",
    "doc = nlp(\"I'd say that Facebook is evil. – Facebook is pretty cool, right?\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "# Serve visualization of sentences containing match with displaCy\n",
    "# set manual=True to make displaCy render straight from a dictionary\n",
    "# (if you're not running the code within a Jupyter environment, you can\n",
    "# use displacy.serve instead)\n",
    "displacy.render(matched_sents, style=\"ent\", manual=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"dddd\"},\n",
    " {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{\"ORTH\": \"+\"}, {\"ORTH\": \"49\"}, {\"ORTH\": \"(\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\"},\n",
    " {\"ORTH\": \")\", \"OP\": \"?\"}, {\"SHAPE\": \"dddd\", \"LENGTH\": 6}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "pattern = [{\"ORTH\": \"(\"}, {\"SHAPE\": \"ddd\"}, {\"ORTH\": \")\"}, {\"SHAPE\": \"ddd\"},\n",
    "           {\"ORTH\": \"-\", \"OP\": \"?\"}, {\"SHAPE\": \"ddd\"}]\n",
    "matcher.add(\"PHONE_NUMBER\", [pattern])\n",
    "\n",
    "doc = nlp(\"Call me at (123) 456 789 or (123) 456 789!\")\n",
    "print([t.text for t in doc])\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = English()  # We only want the tokenizer, so no need to load a pipeline\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "pos_emoji = [\"😀\", \"😃\", \"😂\", \"🤣\", \"😊\", \"😍\"]  # Positive emoji\n",
    "neg_emoji = [\"😞\", \"😠\", \"😩\", \"😢\", \"😭\", \"😒\"]  # Negative emoji\n",
    "\n",
    "# Add patterns to match one or more emoji tokens\n",
    "pos_patterns = [[{\"ORTH\": emoji}] for emoji in pos_emoji]\n",
    "neg_patterns = [[{\"ORTH\": emoji}] for emoji in neg_emoji]\n",
    "\n",
    "# Function to label the sentiment\n",
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n",
    "        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n",
    "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
    "        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n",
    "\n",
    "matcher.add(\"HAPPY\", pos_patterns, on_match=label_sentiment)  # Add positive pattern\n",
    "matcher.add(\"SAD\", neg_patterns, on_match=label_sentiment)  # Add negative pattern\n",
    "\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n",
    "\n",
    "doc = nlp(\"Hello world 😀 #MondayMotivation\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    string_id = doc.vocab.strings[match_id]  # Look up string ID\n",
    "    span = doc[start:end]\n",
    "    print(string_id, span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji  # Installation: pip install emoji\n",
    "from spacy.tokens import Span  # Get the global Span object\n",
    "\n",
    "Span.set_extension(\"emoji_desc\", default=None)  # Register the custom attribute\n",
    "\n",
    "def label_sentiment(matcher, doc, i, matches):\n",
    "    match_id, start, end = matches[i]\n",
    "    if doc.vocab.strings[match_id] == \"HAPPY\":  # Don't forget to get string!\n",
    "        doc.sentiment += 0.1  # Add 0.1 for positive sentiment\n",
    "    elif doc.vocab.strings[match_id] == \"SAD\":\n",
    "        doc.sentiment -= 0.1  # Subtract 0.1 for negative sentiment\n",
    "    span = doc[start:end]\n",
    "    # Verify if it is an emoji and set the extension attribute correctly.\n",
    "    if emoji.is_emoji(span[0].text):\n",
    "        span._.emoji_desc = emoji.demojize(span[0].text, delimiters=(\"\", \"\"), language=doc.lang_).replace(\"_\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Token\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add pattern for valid hashtag, i.e. '#' plus any ASCII token\n",
    "matcher.add(\"HASHTAG\", [[{\"ORTH\": \"#\"}, {\"IS_ASCII\": True}]])\n",
    "\n",
    "# Register token extension\n",
    "Token.set_extension(\"is_hashtag\", default=False)\n",
    "\n",
    "doc = nlp(\"Hello world 😀 #MondayMotivation\")\n",
    "matches = matcher(doc)\n",
    "hashtags = []\n",
    "for match_id, start, end in matches:\n",
    "    if doc.vocab.strings[match_id] == \"HASHTAG\":\n",
    "        hashtags.append(doc[start:end])\n",
    "with doc.retokenize() as retokenizer:\n",
    "    for span in hashtags:\n",
    "        retokenizer.merge(span)\n",
    "        for token in span:\n",
    "            token._.is_hashtag = True\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token._.is_hashtag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "terms = [\"Barack Obama\", \"Angela Merkel\", \"Washington, D.C.\"]\n",
    "# Only run nlp.make_doc to speed things up\n",
    "patterns = [nlp.make_doc(text) for text in terms]\n",
    "matcher.add(\"TerminologyList\", patterns)\n",
    "\n",
    "doc = nlp(\"German Chancellor Angela Merkel and US President Barack Obama \"\n",
    "          \"converse in the Oval Office inside the White House in Washington, D.C.\")\n",
    "matches = matcher(doc)\n",
    "for match_id, start, end in matches:\n",
    "    span = doc[start:end]\n",
    "    print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")\n",
    "patterns = [nlp.make_doc(name) for name in [\"Angela Merkel\", \"Barack Obama\"]]\n",
    "matcher.add(\"Names\", patterns)\n",
    "\n",
    "doc = nlp(\"angela merkel and us president barack Obama\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on lowercase token text:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "from spacy.matcher import PhraseMatcher\n",
    "\n",
    "nlp = English()\n",
    "matcher = PhraseMatcher(nlp.vocab, attr=\"SHAPE\")\n",
    "matcher.add(\"IP\", [nlp(\"127.0.0.1\"), nlp(\"127.127.0.0\")])\n",
    "\n",
    "doc = nlp(\"Often the router will have an IP address such as 192.168.1.1 or 192.168.2.1.\")\n",
    "for match_id, start, end in matcher(doc):\n",
    "    print(\"Matched based on token shape:\", doc[start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "pattern = [\n",
    "  {\n",
    "    \"RIGHT_ID\": \"anchor_founded\",       # unique name\n",
    "    \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}  # token pattern for \"founded\"\n",
    "  }\n",
    "]\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "doc = nlp(\"Smith founded two companies.\")\n",
    "matches = matcher(doc)\n",
    "print(matches) # [(4851363122962674176, [1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"anchor_founded\",\n",
    "        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_subject\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n",
    "    }\n",
    "    # ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2\n",
    "pattern = [\n",
    "    #...\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n",
    "    }\n",
    "    # ...\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3\n",
    "pattern = [\n",
    "    # ...\n",
    "    {\n",
    "        \"LEFT_ID\": \"founded_object\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object_modifier\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.matcher import DependencyMatcher\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "matcher = DependencyMatcher(nlp.vocab)\n",
    "\n",
    "pattern = [\n",
    "    {\n",
    "        \"RIGHT_ID\": \"anchor_founded\",\n",
    "        \"RIGHT_ATTRS\": {\"ORTH\": \"founded\"}\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_subject\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"nsubj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"anchor_founded\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": \"dobj\"},\n",
    "    },\n",
    "    {\n",
    "        \"LEFT_ID\": \"founded_object\",\n",
    "        \"REL_OP\": \">\",\n",
    "        \"RIGHT_ID\": \"founded_object_modifier\",\n",
    "        \"RIGHT_ATTRS\": {\"DEP\": {\"IN\": [\"amod\", \"compound\"]}},\n",
    "    }\n",
    "]\n",
    "\n",
    "matcher.add(\"FOUNDED\", [pattern])\n",
    "doc = nlp(\"Lee, an experienced CEO, has founded two AI startups.\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "print(matches) # [(4851363122962674176, [6, 0, 10, 9])]\n",
    "# Each token_id corresponds to one pattern dict\n",
    "match_id, token_ids = matches[0]\n",
    "for i in range(len(token_ids)):\n",
    "    print(pattern[i][\"RIGHT_ID\"] + \":\", doc[token_ids[i]].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\", config={\"validate\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en import English\n",
    "\n",
    "nlp = English()\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\", \"id\": \"apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}], \"id\": \"san-francisco\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"fran\"}], \"id\": \"san-francisco\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc1 = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc1.ents])\n",
    "\n",
    "doc2 = nlp(\"Apple is opening its first big office in San Fran.\")\n",
    "print([(ent.text, ent.label_, ent.ent_id_) for ent in doc2.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns.jsonl\n",
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler.to_disk(\"./patterns.jsonl\")\n",
    "new_ruler = nlp.add_pipe(\"entity_ruler\").from_disk(\"./patterns.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.add_patterns([{\"label\": \"ORG\", \"pattern\": \"Apple\"}])\n",
    "nlp.to_disk(\"/path/to/pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "patterns = [{\"label\": \"TEST\", \"pattern\": str(i)} for i in range(100000)]\n",
    "with nlp.select_pipes(enable=\"tagger\"):\n",
    "    ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ruler = nlp.add_pipe(\"span_ruler\")\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"Apple\"},\n",
    "            {\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"Apple is opening its first big office in San Francisco.\")\n",
    "print([(span.text, span.label_) for span in doc.spans[\"ruler\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# only annotate doc.ents, not doc.spans\n",
    "config = {\"spans_key\": None, \"annotate_ents\": True, \"overwrite\": False}\n",
    "ruler = nlp.add_pipe(\"span_ruler\", config=config)\n",
    "patterns = [{\"label\": \"ORG\", \"pattern\": \"MyCorp Inc.\"}]\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "doc = nlp(\"MyCorp Inc. is a company in the U.S.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# patterns.jsonl\n",
    "{\"label\": \"ORG\", \"pattern\": \"Apple\"}\n",
    "{\"label\": \"GPE\", \"pattern\": [{\"LOWER\": \"san\"}, {\"LOWER\": \"francisco\"}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "\n",
    "patterns = srsly.read_jsonl(\"patterns.jsonl\")\n",
    "ruler = nlp.add_pipe(\"span_ruler\")\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "\n",
    "@Language.component(\"expand_person_entities\")\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check for title if it's a person and not the first token\n",
    "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@Language.component(\"expand_person_entities\")\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ == \"PERSON\" and ent.start != 0:\n",
    "            prev_token = doc[ent.start - 1]\n",
    "            if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                new_ents.append(new_ent)\n",
    "            else:\n",
    "                new_ents.append(ent)\n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "# Add the component after the named entity recognizer\n",
    "nlp.add_pipe(\"expand_person_entities\", after=\"ner\")\n",
    "\n",
    "doc = nlp(\"Dr. Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Span\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def get_person_title(span):\n",
    "    if span.label_ == \"PERSON\" and span.start != 0:\n",
    "        prev_token = span.doc[span.start - 1]\n",
    "        if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "            return prev_token.text\n",
    "\n",
    "# Register the Span extension as 'person_title'\n",
    "Span.set_extension(\"person_title\", getter=get_person_title)\n",
    "\n",
    "doc = nlp(\"Dr Alex Smith chaired first board meeting of Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_, ent._.person_title) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "for ent in person_entities:\n",
    "    # Because the entity is a span, we need to use its root token. The head\n",
    "    # is the syntactic governor of the person, e.g. the verb\n",
    "    head = ent.root.head\n",
    "    if head.lemma_ == \"work\":\n",
    "        # Check if the children contain a preposition\n",
    "        preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "        for prep in preps:\n",
    "            # Check if tokens part of ORG entities are in the preposition's\n",
    "            # children, e.g. at -> Acme Corp Inc.\n",
    "            orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "            # If the verb is in past tense, the company was a previous company\n",
    "            print({\"person\": ent, \"orgs\": orgs, \"past\": head.tag_ == \"VBD\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.language import Language\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "@Language.component(\"extract_person_orgs\")\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [token for token in prep.children if token.ent_type_ == \"ORG\"]\n",
    "                print({'person': ent, 'orgs': orgs, 'past': head.tag_ == \"VBD\"})\n",
    "    return doc\n",
    "\n",
    "# To make the entities easier to work with, we'll merge them into single tokens\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "nlp.add_pipe(\"extract_person_orgs\")\n",
    "\n",
    "doc = nlp(\"Alex Smith worked at Acme Corp Inc.\")\n",
    "# If you're not in a Jupyter / IPython environment, use displacy.serve\n",
    "displacy.render(doc, options={\"fine_grained\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.component(\"extract_person_orgs\")\n",
    "def extract_person_orgs(doc):\n",
    "    person_entities = [ent for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "    for ent in person_entities:\n",
    "        head = ent.root.head\n",
    "        if head.lemma_ == \"work\":\n",
    "            preps = [token for token in head.children if token.dep_ == \"prep\"]\n",
    "            for prep in preps:\n",
    "                orgs = [t for t in prep.children if t.ent_type_ == \"ORG\"]\n",
    "                aux = [token for token in head.children if token.dep_ == \"aux\"]\n",
    "                past_aux = any(t.tag_ == \"VBD\" for t in aux)\n",
    "                past = head.tag_ == \"VBD\" or head.tag_ == \"VBG\" and past_aux\n",
    "                print({'person': ent, 'orgs': orgs, 'past': past})\n",
    "    return doc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
